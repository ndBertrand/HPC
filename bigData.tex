\section{Le bigdata}
Le terme de Big data est définit par la plupart des scientifiques et experts en données avec trois caractéristiques principales (appelées les 3V)(\cite{kacfah_emani_understandable_2015}) :
\begin{itemize}
    \item Le Volume : des volumes immenses de données numériques sont générées en continu par des millions d’appareils et applications connectes.
    
    \item La Vitesse ou vélocité : les données sont générées très rapidement et doivent par conséquent être traité de façon rapide pour en extraire les informations utiles et pertinentes.
    
    \item La Variété : les données sont issues de diverses sources et se présentent sous de multiples formats (texte, documents, vidéos, etc.) et peuvent être structurées ou non structurées, publiques ou privées, complet ou incomplet etc 
\end{itemize}
\cite{oussous_big_2018}
Le terme big Data fait donc référence à de gigantesques volumes de données dont la taille dépasse les capacités des outils logiciels de base de données classiques. Pour cela il a fallu inventer de nouvelles technologies pour pouvoir traiter cette immense quantité de données. Cette ainsi que voit le jour le MapReduce et par la suite le Hadoop. Originairement mis en place par Google pour résoudre les problèmes d’indexation des pages web, MapReduce est aujourd’hui le model algorithmique le plus utilisé dans le traitement des très grands volumes de données.\cite{kacfah_emani_understandable_2015} \cite{gandomi_beyond_2015} 