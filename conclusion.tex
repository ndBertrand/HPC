\section{conclusion}
Le Framework Hadoop propose un stockage fiable évolutif et partagé avec un traitement distribué à grande échelle. Il utilise deux composants principaux à savoir le MapReduce pour le traitement et le HDFS comme système de fichiers distribues. Les deux architectures sont parfaitement adaptées pour traiter de grands volumes de données. Mais, malgré ses nombreux avantages, la solution que propose Hadoop ne convient pas forcement à toutes les organisations. Par exemple pour des petits volumes de données, ses bénéfices seront quasi-inexistante bien qu’il y ait le besoin d’une analyse poussée de ces données. L’inconvénient majeur reste quand même l’apprentissage qui demande un temps assez considérable pour pouvoir construire un cluster et à réaliser l’analyse requise des données.
