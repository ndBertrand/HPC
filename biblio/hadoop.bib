
@misc{jolia-ferrier_livre_nodate,
	title = {Livre {Big} {Data} - {Concepts} et mise en oeuvre de {Hadoop}},
	url = {https://www.editions-eni.fr/livre/big-data-concepts-et-mise-en-oeuvre-de-hadoop-9782746086883},
	urldate = {2018-11-06},
	author = {JOLIA-FERRIER, Laurent},
	file = {Livre Big Data - Concepts et mise en oeuvre de Hadoop:C\:\\Users\\bertr\\Zotero\\storage\\PNQSRDM9\\big-data-concepts-et-mise-en-oeuvre-de-hadoop-9782746086883.html:text/html}
}

@misc{chokogoue_livre_nodate,
	title = {Livre {Hadoop} - {Devenez} opérationnel dans le monde du {Big} {Data}},
	url = {https://www.editions-eni.fr/livre/hadoop-devenez-operationnel-dans-le-monde-du-big-data-9782409007613},
	urldate = {2018-11-06},
	author = {CHOKOGOUE, Juvénal},
	file = {Livre Hadoop - Devenez opérationnel dans le monde du Big Data:C\:\\Users\\bertr\\Zotero\\storage\\S3H665GQ\\hadoop-devenez-operationnel-dans-le-monde-du-big-data-9782409007613.html:text/html}
}

@article{polato_comprehensive_2014,
	title = {A comprehensive view of {Hadoop} research—{A} systematic literature review},
	volume = {46},
	issn = {1084-8045},
	url = {http://www.sciencedirect.com/science/article/pii/S1084804514001635},
	doi = {10.1016/j.jnca.2014.07.022},
	abstract = {Context: In recent years, the valuable knowledge that can be retrieved from petabyte scale datasets – known as Big Data – led to the development of solutions to process information based on parallel and distributed computing. Lately, Apache Hadoop has attracted strong attention due to its applicability to Big Data processing. Problem: The support of Hadoop by the research community has provided the development of new features to the framework. Recently, the number of publications in journals and conferences about Hadoop has increased consistently, which makes it difficult for researchers to comprehend the full body of research and areas that require further investigation. Solution: We conducted a systematic literature review to assess research contributions to Apache Hadoop. Our objective was to identify gaps, providing motivation for new research, and outline collaborations to Apache Hadoop and its ecosystem, classifying and quantifying the main topics addressed in the literature. Results: Our analysis led to some relevant conclusions: many interesting solutions developed in the studies were never incorporated into the framework; most publications lack sufficient formal documentation of the experiments conducted by authors, hindering their reproducibility; finally, the systematic review presented in this paper demonstrates that Hadoop has evolved into a solid platform to process large datasets, but we were able to spot promising areas and suggest topics for future research within the framework.},
	urldate = {2018-11-03},
	journal = {Journal of Network and Computer Applications},
	author = {Polato, Ivanilton and Ré, Reginaldo and Goldman, Alfredo and Kon, Fabio},
	month = nov,
	year = {2014},
	keywords = {MapReduce, Apache Hadoop, HDFS, Survey, Systematic literature review},
	pages = {1--25},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bertr\\Zotero\\storage\\DZXDDVI6\\Polato et al. - 2014 - A comprehensive view of Hadoop research—A systemat.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\bertr\\Zotero\\storage\\NQ9KM9TC\\S1084804514001635.html:text/html}
}

@article{borthakur_hadoop_2005,
	title = {The {Hadoop} {Distributed} {File} {System}: {Architecture} and {Design}},
	language = {en},
	author = {Borthakur, Dhruba},
	year = {2005},
	pages = {12},
	file = {Borthakur - 2005 - The Hadoop Distributed File System Architecture a.pdf:C\:\\Users\\bertr\\Zotero\\storage\\DW3GDQ35\\Borthakur - 2005 - The Hadoop Distributed File System Architecture a.pdf:application/pdf}
}

@article{uzunkaya_hadoop_2015,
	title = {Hadoop {Ecosystem} and {Its} {Analysis} on {Tweets}},
	volume = {195},
	issn = {18770428},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877042815039087},
	doi = {10.1016/j.sbspro.2015.06.429},
	abstract = {Hadoop is Java based programming framework for distributed storage and processing of large data sets on commodity hardware. It is developed by Apache Software Foundation as open source framework. Hadoop basically has two main components. First one is Hadoop Distributed File System (HDFS) for distributed storage and second part is MapReduce for distributed processing. HDFS is a file system which builds on the existing file system. It is Java-based sub project of Apache Hadoop. HDFS provides scalable and reliable data storage on commandity hardware. A master/slave architecture is used by HDFS. In this architecture, HDFS has a single NameNode and more than one DataNodes. The NameNode manages the file system and stores the metadata. It acts like a file manager on HDFS. Because all files and directories are represented on the NameNode. DataNodes stores the part of data. A file is splited into one or more blocks (default 64MB or 128MB) and that blocks are stored in DataNodes. MapReduce is a programming model which is used for processing and generating large data sets with a parallel, distributed algorithm on a cluster. A MapReduce job generally splits the input data set into independent blocks which are processed by the map tasks in a completely parallel manner. First step is mapping of data set in MapReduce architecture. The framework sorts the outputs of the mapping process, which are then input to the second step is reduce task. Input and the output of the job are stored in a file-system. The MapReduce framework consists of two process which are JobTracker and TaskTracker. The JobTracker manages the resources that are TaskTracker. The TaskTracker is a processing node in the cluster. It accepts several tasks like map reduce and shuffle from a Job Tracker. Twitter4J is an unofficial Java library for the Twitter application programming interface. It is integrated Java application with the all Twitter services. This paper focuses on Hadoop and its ecosystem and implementation Hadoop based platform for analyzing on collected tweets. The regarding analyzed results are transferred to graphical charts which is showed on a web page.},
	language = {en},
	urldate = {2018-11-07},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Uzunkaya, Can and Ensari, Tolga and Kavurucu, Yusuf},
	month = jul,
	year = {2015},
	pages = {1890--1897},
	file = {Uzunkaya et al. - 2015 - Hadoop Ecosystem and Its Analysis on Tweets.pdf:C\:\\Users\\bertr\\Zotero\\storage\\A4EXJ4S5\\Uzunkaya et al. - 2015 - Hadoop Ecosystem and Its Analysis on Tweets.pdf:application/pdf}
}


@article{kacfah_emani_understandable_2015,
	title = {Understandable {Big} {Data}: {A} survey},
	volume = {17},
	issn = {1574-0137},
	shorttitle = {Understandable {Big} {Data}},
	url = {http://www.sciencedirect.com/science/article/pii/S1574013715000064},
	doi = {10.1016/j.cosrev.2015.05.002},
	abstract = {This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.},
	urldate = {2018-11-03},
	journal = {Computer Science Review},
	author = {Kacfah Emani, Cheikh and Cullot, Nadine and Nicolle, Christophe},
	month = aug,
	year = {2015},
	keywords = {Big data, Coreference resolution, Entity linking, Hadoop, Information extraction, Ontology alignment, Reasoning},
	pages = {70--81},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bertr\\Zotero\\storage\\7IKJ6BIR\\Kacfah Emani et al. - 2015 - Understandable Big Data A survey.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\bertr\\Zotero\\storage\\BYJRYTT3\\S1574013715000064.html:text/html}
}

@article{gandomi_beyond_2015,
	title = {Beyond the hype: {Big} data concepts, methods, and analytics},
	volume = {35},
	issn = {0268-4012},
	shorttitle = {Beyond the hype},
	url = {http://www.sciencedirect.com/science/article/pii/S0268401214001066},
	doi = {10.1016/j.ijinfomgt.2014.10.007},
	abstract = {Size is the first, and at times, the only dimension that leaps out at the mention of big data. This paper attempts to offer a broader definition of big data that captures its other unique and defining characteristics. The rapid evolution and adoption of big data by industry has leapfrogged the discourse to popular outlets, forcing the academic press to catch up. Academic journals in numerous disciplines, which will benefit from a relevant discussion of big data, have yet to cover the topic. This paper presents a consolidated description of big data by integrating definitions from practitioners and academics. The paper's primary focus is on the analytic methods used for big data. A particular distinguishing feature of this paper is its focus on analytics related to unstructured data, which constitute 95\% of big data. This paper highlights the need to develop appropriate and efficient analytical methods to leverage massive volumes of heterogeneous data in unstructured text, audio, and video formats. This paper also reinforces the need to devise new tools for predictive analytics for structured big data. The statistical methods in practice were devised to infer from sample data. The heterogeneity, noise, and the massive size of structured big data calls for developing computationally efficient algorithms that may avoid big data pitfalls, such as spurious correlation.},
	number = {2},
	urldate = {2018-11-03},
	journal = {International Journal of Information Management},
	author = {Gandomi, Amir and Haider, Murtaza},
	month = apr,
	year = {2015},
	keywords = {Big data analytics, Big data definition, Predictive analytics, Unstructured data analytics},
	pages = {137--144},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bertr\\Zotero\\storage\\6QB8KQ8Y\\Gandomi et Haider - 2015 - Beyond the hype Big data concepts, methods, and a.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\bertr\\Zotero\\storage\\PL79BT8Z\\S0268401214001066.html:text/html}
}

@article{oussous_big_2018,
	title = {Big {Data} technologies: {A} survey},
	volume = {30},
	issn = {1319-1578},
	shorttitle = {Big {Data} technologies},
	url = {http://www.sciencedirect.com/science/article/pii/S1319157817300034},
	doi = {10.1016/j.jksuci.2017.06.001},
	abstract = {Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.},
	number = {4},
	urldate = {2018-11-03},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Oussous, Ahmed and Benjelloun, Fatima-Zahra and Ait Lahcen, Ayoub and Belfkih, Samir},
	month = oct,
	year = {2018},
	keywords = {Hadoop, Big Data, Big Data analytics, Big Data distributions, Machine learning, NoSQL},
	pages = {431--448},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bertr\\Zotero\\storage\\P9DVWTRP\\Oussous et al. - 2018 - Big Data technologies A survey.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\bertr\\Zotero\\storage\\E5939WX5\\S1319157817300034.html:text/html}
}

@article{odriscoll_big_2013,
	title = {‘{Big} data’, {Hadoop} and cloud computing in genomics},
	volume = {46},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046413001007},
	doi = {10.1016/j.jbi.2013.07.001},
	abstract = {Since the completion of the Human Genome project at the turn of the Century, there has been an unprecedented proliferation of genomic sequence data. A consequence of this is that the medical discoveries of the future will largely depend on our ability to process and analyse large genomic data sets, which continue to expand as the cost of sequencing decreases. Herein, we provide an overview of cloud computing and big data technologies, and discuss how such expertise can be used to deal with biology’s big data sets. In particular, big data technologies such as the Apache Hadoop project, which provides distributed and parallelised data processing and analysis of petabyte (PB) scale data sets will be discussed, together with an overview of the current usage of Hadoop within the bioinformatics community.},
	language = {en},
	number = {5},
	urldate = {2018-11-01},
	journal = {Journal of Biomedical Informatics},
	author = {O’Driscoll, Aisling and Daugelaite, Jurate and Sleator, Roy D.},
	month = oct,
	year = {2013},
	pages = {774--781},
	file = {O’Driscoll et al. - 2013 - ‘Big data’, Hadoop and cloud computing in genomics.pdf:C\:\\Users\\bertr\\Zotero\\storage\\W3WNB3YK\\O’Driscoll et al. - 2013 - ‘Big data’, Hadoop and cloud computing in genomics.pdf:application/pdf}
}

@article{dean_mapreduce:_2008,
	title = {{MapReduce}: simplified data processing on large clusters},
	volume = {51},
	issn = {00010782},
	shorttitle = {{MapReduce}},
	url = {http://portal.acm.org/citation.cfm?doid=1327452.1327492},
	doi = {10.1145/1327452.1327492},
	abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
	language = {en},
	number = {1},
	urldate = {2018-11-06},
	journal = {Communications of the ACM},
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	month = jan,
	year = {2008},
	pages = {107},
	file = {Dean et Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf:C\:\\Users\\bertr\\Zotero\\storage\\RNURHA6J\\Dean et Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf:application/pdf}
}

@article{sardar_partition_2018,
	title = {Partition based clustering of large datasets using {MapReduce} framework: {An} analysis of recent themes and directions},
	issn = {2314-7288},
	shorttitle = {Partition based clustering of large datasets using {MapReduce} framework},
	url = {http://www.sciencedirect.com/science/article/pii/S231472881730065X},
	doi = {10.1016/j.fcij.2018.06.002},
	abstract = {Data clustering is one of the fundamental techniques in scientific analysis and data mining, which describes a dataset according to similarities among its objects. Partition based clustering algorithms are the most popular and widely used clustering technique. In this information era, due to the digitization of every field, the huge volume of data is available to data analysts. The quick growth of such datasets makes decade old computing platforms, programming paradigms, and clustering algorithms become inadequate to obtain knowledge from these datasets. To cluster such large datasets, Hadoop distributed platform, MapReduce programming paradigm and modified clustering algorithms are being used to shrink the computational time by distributing clustering job across multiple computing nodes. This paper provides a comprehensive review of Hadoop and MapReduce and their components. This paper aims to survey recent research works on partition based clustering algorithms which use MapReduce as their programming paradigm. In many recent works, the traditional partition based clustering algorithms like K-means, K-prototypes, K-medoids, K-modes and Fuzzy C-means are modified for MapReduce paradigm in order to obtain different clustering objectives on different datasets for reducing the computational time. The contribution of this paper is (1) to provide an overview of clustering challenges in real world large dataset clustering and the role of MapReduce programming paradigm and its supporting platforms in dealing the challenges for several tasks in different datasets and (2) to review recent works in partition based clustering using MapReduce paradigm for different clustering objectives for different datasets employing different strategies.},
	urldate = {2018-11-03},
	journal = {Future Computing and Informatics Journal},
	author = {Sardar, Tanvir Habib and Ansari, Zahid},
	month = jun,
	year = {2018},
	keywords = {Hadoop, Data mining, MapReduce, Partition-based clustering},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bertr\\Zotero\\storage\\5CXNW3LM\\Sardar et Ansari - 2018 - Partition based clustering of large datasets using.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\bertr\\Zotero\\storage\\W6H9EZII\\S231472881730065X.html:text/html}
}

@article{derbeko_security_2016,
	title = {Security and privacy aspects in {MapReduce} on clouds: {A} survey},
	volume = {20},
	issn = {1574-0137},
	shorttitle = {Security and privacy aspects in {MapReduce} on clouds},
	url = {http://www.sciencedirect.com/science/article/pii/S157401371530006X},
	doi = {10.1016/j.cosrev.2016.05.001},
	abstract = {MapReduce is a programming system for distributed processing of large-scale data in an efficient and fault tolerant manner on a private, public, or hybrid cloud. MapReduce is extensively used daily around the world as an efficient distributed computation tool for a large class of problems, e.g., search, clustering, log analysis, different types of join operations, matrix multiplication, pattern matching, and analysis of social networks. Security and privacy of data and MapReduce computations are essential concerns when a MapReduce computation is executed in public or hybrid clouds. In order to execute a MapReduce job in public and hybrid clouds, authentication of mappers–reducers, confidentiality of data-computations, integrity of data-computations, and correctness–freshness of the outputs are required. Satisfying these requirements shields the operation from several types of attacks on data and MapReduce computations. In this paper, we investigate and discuss security and privacy challenges and requirements, considering a variety of adversarial capabilities, and characteristics in the scope of MapReduce. We also provide a review of existing security and privacy protocols for MapReduce and discuss their overhead issues.},
	urldate = {2018-11-07},
	journal = {Computer Science Review},
	author = {Derbeko, Philip and Dolev, Shlomi and Gudes, Ehud and Sharma, Shantanu},
	month = may,
	year = {2016},
	keywords = {Cloud computing, Distributed computing, Hadoop, HDFS, Hybrid cloud, MapReduce algorithms, Privacy, Private cloud, Public cloud, Security},
	pages = {1--28},
	file = {ScienceDirect Snapshot:C\:\\Users\\bertr\\Zotero\\storage\\BSZXGTDR\\S157401371530006X.html:text/html;Submitted Version:C\:\\Users\\bertr\\Zotero\\storage\\29XAX2CI\\Derbeko et al. - 2016 - Security and privacy aspects in MapReduce on cloud.pdf:application/pdf}
}