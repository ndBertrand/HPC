\section{Introduction}
Aujourd’hui, d’énormes volumes de données sont produits de façons quotidiennes dans de domaines très variés comme le commerce, l’alimentation le sport etc. Ces immenses volumes de données qui dépassent la capacité des systèmes conventionnelle de traitement constituent ce qu’on l’on peut qualifier aujourd’hui de Big Data. Il est souvent envisagé de recourir au cloud pour l’analyse des big data, mais l’inconvénient majeur est la latence que présente le cloud. En effet ces données doivent pouvoir être traité dans des temps très court voir même du temps réel pour qu’elles soient vraiment utiles. L’analyse des Big Data requiert donc de puissants algorithmes afin de rendre très rapidement compréhensibles ces données et ainsi pouvoir les exploiter. C’est là que le Big Data croise le calcule de haute performance (HPC).\cite{odriscoll_big_2013} Dans ce travaille, nous parlons d’Hadoop qui est un Framework conçu pour faciliter la gestion des grands volumes de données. Nous commençons ainsi par rappeler ce qu’est le Big Data et ses notions de Bases. Nous montrons ensuite le plus connu et le plus utilisé des modèles algorithmiques pour la gestion des Big Data qui est le MapReduce. Nous finissons ensuite par le plus populaire de ses implémentations à savoir l’Hadoop : ses origines, son évolution, son système de Fichier HDFS et ses distributions.